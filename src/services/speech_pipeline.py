from __future__ import annotations

from typing import Any, Dict, List, Literal, Generator
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
from src.models.stt_whisper import (
    get_whisperx_models,
    extract_word_timings,
)

from src.models.pitch_crepe import extract_pitch_crepe
from src.models.align_merge import merge_words_with_pitch_curve
from src.models.g2p import text_to_phonemes
from src.models.pronunciation import phonemes_to_hangul_ipa

Mode = Literal["pron", "inton", "all"]


def _process_pronunciation(words: List[str]) -> Dict[str, Any]:
    """ë°œìŒ ë¶„ì„ ì‹¤í–‰"""
    pron: Dict[str, Any] = {}

    for idx, w in enumerate(words):
        # 1) ë‹¨ì–´ë³„ phonemes (ARPAbet) -> upl
        upl = text_to_phonemes(w)

        # 2) ë‹¨ì–´ë³„ í•œê¸€/IPA
        ukor, _ipa_str, uipa = phonemes_to_hangul_ipa(upl)

        # ë™ì¼ ë‹¨ì–´ ë°˜ë³µ ëŒ€ë¹„: keyë¥¼ ìœ ë‹ˆí¬í•˜ê²Œ
        key = w  # ì¤‘ë³µ ê°€ëŠ¥ì„± ìˆìœ¼ë©´: f"{w}#{idx}"ë¡œ ë³€ê²½

        pron[key] = {
            "upl": upl,     # ARPAbet list
            "uipa": uipa,   # IPA list
            "ukor": ukor,   # í•œê¸€ ë°œìŒ ë¬¸ìì—´
        }

    return pron


def _process_intonation(
    audio_path: str, 
    word_segments: List[Dict[str, Any]], 
    device: str
) -> List[Dict[str, Any]]:
    """ì¸í† ë„¤ì´ì…˜ ë¶„ì„ ì‹¤í–‰"""
    pitch_result = extract_pitch_crepe(audio_path, device=device)
    return merge_words_with_pitch_curve(word_segments, pitch_result)


def analyze_speech_stream(
    audio_path: str,
    # whisper_model_name: str = "small.en",
    # whisper_vad_method: str = "silero",
    loaded_models: tuple,   # ì´ë¯¸ ë¡œë”©ëœ ëª¨ë¸ì„ ë°›ìŒ
    mode: Mode = "all",
) -> Generator[str, None, None]:
    """
    [ê¸°ëŠ¥] ìŒì„± íŒŒì´í”„ë¼ì¸ ë©”ì¸ í•¨ìˆ˜
    1. WhisperX (ê³µí†µ)
    2. Pronunciation (G2P)
    3. Intonation (CREPE)
    """
    
    # 1. WhisperX ê³µí†µ ë‹¨ê³„: ëª¨ë¸ ë¡œë“œ -> ì‹¤í–‰ -> ë©”ëª¨ë¦¬ ì •ë¦¬
    # model, model_a, metadata, device = get_whisperx_models(
    #     model_name=whisper_model_name,
    #     vad_method=whisper_vad_method,
    # )
    model, model_a, metadata, device = loaded_models
    
    try:
        word_segments = extract_word_timings(
            audio_path=audio_path,
            model=model,
            model_a=model_a,
            metadata=metadata,
            device=device,
            batch_size=16,
        )
    except Exception as e:
        yield json.dumps({"type": "error", "message": str(e)}) + "\n"
        return

    words = [w["word"] for w in word_segments]

    # ì‹¤í–‰í•  ì‘ì—… í”Œë˜ê·¸ ì„¤ì •
    do_pron = mode in ("pron", "all")
    do_into = mode in ("inton", "all")

    # 2. ë¶„ì„ ì‹¤í–‰
    with ThreadPoolExecutor(max_workers=2) as executor:
        future_map = {}

        if do_pron:
            # ë°œìŒ ë¶„ì„ ì‘ì—… ì œì¶œ
            f_pron = executor.submit(_process_pronunciation, words)
            future_map[f_pron] = "pron"
        
        if do_into:
            # ì¸í† ë„¤ì´ì…˜ ë¶„ì„ ì‘ì—… ì œì¶œ
            f_into = executor.submit(_process_intonation, audio_path, word_segments, device)
            future_map[f_into] = "inton"

        # ë¨¼ì € ëë‚˜ëŠ” ì‘ì—…ë¶€í„° yield (as_completed)
        for future in as_completed(future_map):
            task_type = future_map[future]
            try:
                result_data = future.result()
                
                # ê²°ê³¼ ì „ì†¡ (typeìœ¼ë¡œ êµ¬ë¶„)
                yield json.dumps({
                    "type": task_type,
                    "data": result_data
                }, ensure_ascii=False) + "\n"
                
            except Exception as e:
                yield json.dumps({
                    "type": "error",
                    "task": task_type,
                    "message": str(e)
                }, ensure_ascii=False) + "\n"

# ë¡œì»¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    import json
    import os
    # ëª¨ë¸ ë¡œë” í•¨ìˆ˜ import í•„ìš”
    from src.models.stt_whisper import get_whisperx_models

    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ í™•ì¸
    test_file = "./experiments/wav_data/i_like_to_dance.wav"
    
    if os.path.exists(test_file):
        print("â³ [Test] ëª¨ë¸ ë¡œë”© ì¤‘... (ì²˜ìŒì—” ì‹œê°„ì´ ì¢€ ê±¸ë¦½ë‹ˆë‹¤)")
        
        # 1. í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì—¬ê¸°ì„œ ëª¨ë¸ì„ ì§ì ‘ ë¡œë“œí•©ë‹ˆë‹¤. (Main.pyì˜ lifespan ì—­í• )
        # ì‹¤ì œ ì„œë²„ì—ì„œëŠ” ì´ë¯¸ ë¡œë“œëœ ê±¸ ì“°ì§€ë§Œ, ë¡œì»¬ í…ŒìŠ¤íŠ¸ì—ì„  ì§ì ‘ ì¤€ë¹„í•´ì•¼ í•©ë‹ˆë‹¤.
        loaded_models_tuple = get_whisperx_models(
            model_name="small.en", 
            vad_method="silero"
        )
        print("âœ… [Test] ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

        print("ğŸ¤ [Test] ë¶„ì„ ì‹œì‘...")
        
        # 2. ë¡œë“œëœ ëª¨ë¸ì„ ì¸ìë¡œ ë„˜ê²¨ì¤ë‹ˆë‹¤.
        generator = analyze_speech_stream(
            audio_path=test_file, 
            mode="all", 
            loaded_models=loaded_models_tuple # <--- í•µì‹¬: ëª¨ë¸ ì „ë‹¬
        )
        
        # 3. ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
        for chunk in generator:
            print(chunk.strip())
            
    else:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_file}")